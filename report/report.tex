\documentclass[10pt]{report}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage[left=2cm,top=1.5cm,right=2cm,bottom=2cm]{geometry}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsopn}
\usepackage{amsthm}
\usepackage[hidelinks]{hyperref}
\usepackage{cleveref}
\usepackage{fancyvrb}
\usepackage{tikz}
\usepackage[justification=centering]{caption}
% \patchcmd{\thebibliography}{\chapter*}{\section*}{}{}
\usepackage[super]{nth}
\usepackage{textcomp}
\usepackage{enumitem}
\usepackage{doi}
\usepackage{mathrsfs}
\usepackage{siunitx}
\setlist{nosep}


\theoremstyle{plain}
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{prop}[thm]{Property}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}{Corollary}[thm]

\theoremstyle{definition}
\newtheorem{defn}{Definition}[chapter]
\newtheorem{axi}{Axiom}[chapter]
\newtheorem{eqn}{Equation}[chapter]

\theoremstyle{remark}
\newtheorem*{rem}{Remark}


\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{1}
%\renewcommand\thesection{\arabic{section}}

\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\Rb}{\ensuremath{\overline{\mathbb{R}}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Q}{\ensuremath{\mathbb{Q}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
\newcommand{\U}{\ensuremath{\mathbb{U}}}
\newcommand{\F}{\ensuremath{\mathbb{F}}}
\newcommand{\K}{\ensuremath{\mathbb{K}}}
\newcommand{\TODO}{\textbf{TODO}}


\newcommand\eqdef{\stackrel{\mathclap{\mbox{\tiny def}}}{=}}


\sisetup{inter-unit-product=\ensuremath{{}\cdot{}}}

\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle#1|}
\newcommand{\braket}[2]{\langle#1|#2\rangle}

\newcommand{\dd}{\mathrm{d}}
\renewcommand{\geq}{\geqslant}
\renewcommand{\leq}{\leqslant}

\newcommand{\mat}[1]{\begin{pmatrix}#1\end{pmatrix}}
\newcommand{\bs}{\boldsymbol}

\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\argmax}{arg\,max}
\DeclareMathOperator{\argmin}{arg\,min}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\Span}{Span}


\newcommand{\class}[1]{{\mathscr{C}^{#1}}}







% \usepackage{titlesec}
% \titleformat{\chapter}[hang] 

% \makeatletter
% \def\@makechapterhead#1{%
%   \vspace*{0\p@}% %%% removed!
%   {\parindent \z@ \raggedright \normalfont
%     \ifnum \c@secnumdepth >\m@ne
%         \huge\bfseries \@chapapp\space \thechapter
%         \par\nobreak
%         \vskip 10\p@
%     \fi
%     \interlinepenalty\@M
%     \Huge \bfseries #1\par\nobreak
%     \vskip 30\p@
%   }}
% \def\@makeschapterhead#1{%
%   \vspace*{0\p@}% %%% removed!
%   {\parindent \z@ \raggedright
%     \normalfont
%     \interlinepenalty\@M
%     \Huge \bfseries  #1\par\nobreak
%     \vskip 30\p@
%   }}
% \makeatother

% \usepackage{titlesec}
% \titlespacing*{\section}{0pt}{.7\baselineskip}{.5\baselineskip}
% \titlespacing*{\subsection}{0pt}{.4\baselineskip}{.3\baselineskip}

\begin{document}

\begin{titlepage}
  \centering
  {\scshape\huge École Normale Supérieure \par}
  \vspace{0.3cm}
  {\scshape\Large Department of Mathematics and their Applications (DMA) \par}
  \vspace{3cm}
  {\Huge\bfseries Error estimation in maxlike reconstruction for quantum tomography \par}
  \vspace{0.5cm}
  {\scshape\Large Internship report\par}
  \vspace{3cm}
  {\LARGE Thibaut \textsc{Pérami}\par}
  \vfill
  {
    \large
    supervised by\par
    Igor \textsc{Dotsenko}\par
    and\par
    Pierre \textsc{Rouchon}\par
  in\par
  LKB, Collège de France
  }

  \vfill

  % Bottom of the page
  {\Large \today\par}
\end{titlepage}

\newcommand{\fset}{\ensuremath{\mathop{\text{\textquotesingle}}}}


% \pagenumbering{gobble} % to avoid counting contents page in report
\tableofcontents

\chapter*{Introduction}
% \pagenumbering{arabic}
\addcontentsline{toc}{chapter}{Introduction}

This report details my internship with Pierre Rouchon and Igor dotsenko on
Quantum tomography by maximum-likelihood reconstruction. It was done mainly at
the Laboratory Kessler-Brosel (LKB) in the Collège de France.

The goal of Quantum tomography is to find ways to reconstruct the state of
a quantum system from several direct or indirect measurement on it. In some case
those measurement do not modify the state of the system. Those are called QND
(Quantum Non-Destructive) measurement.

The specific physical project on which I worked with Igor Dotsenko is the
internship project of Luis Najera about thermodynamics in the case of
atom-cavity interaction in quantum optics. More precisely, we send an circular
Rydberg atom in a resonant cavity containing photons. The state of the cavity is
thermal i.e it follow the Bose-Einstein distribution. The atom then interacts
with the cavity. If the atom is in a thermal state between two atomic level that
are at the same frequency than the cavity then a thermal exchange will occur in
the expected way. However if we pump one of the state of the atom in a hidden
state, like a Maxwell demon, we can can apparently break the second law and make
a cold atom give head to a hotter cavity.

In order to study this experiment, we need to measure the state of the cavity.
That state cannot be measured directly, so we do it by quantum tomography using
maximum-likelihood reconstruction. This construction takes into account all the
measurements made and deduce a density matrix. To find it, one needs to solve a
convex optimisation problem on the set of density matrix (hermitian positive
definite matrix of trace 1). This was done using a gradient descent method with
projection of the gradient on the domain.

The deduced state is usually not a pure state because the reconstruction often
came from a state that has been prepared several times.
All the prepared state are slightly different.
Furthermore, other factors like measurements imperfection and decoherent
relaxation decrease the precision of the reconstruction.

With this measurements, we can now get and interpret the results of our Maxwell
demon experiment. However, to be able to interpret measurement, we need to know
the error on the reconstructed state. The original paper [ref here] on
tomography only provide the standard deviation of usual quantum operator which
are linear in the density matrix. However to do our analysis of the second law,
we need to evaluate the error on the entropy.

In order to do that, I had to extract from the reconstruction the probability
distribution around the maximum which is our estimator. But this distribution
cannot be samples easily because its a mix of Gaussian and exponential law
truncated into the domain of density matrix. Into to sample from this
probability I use an adapted hit and run method to build a Markov chain on the
density matrix space (but discrete time) whose stationary distribution is the
wanted distribution. By sampling this chain long enough, I can get the statistic
average and standard deviation of any real function of the density matrix space.

In the end we get nice plots with errorbars and we were able to determine which
point we realistic within the error-bars and which were completely wrong. We
could then redo the bad point and reach a clean conclusion.

\

This report alternate maths and physics. The odd chapters are about the physical
part of the internship and the even ones about maths. The five first chapter are
about my understanding of prior work that I had to do in order to do what I did.
The last two are about my personal contributions.

In the first chapter I
present the experiment, its modelisation and what we want to prove. In the
second I explain the basics of maximum likelihood reconstruction then I show in
the third chapter how to apply this method for our particular case. In the
fourth chapter I show how to solve the convex optimization problem. Then I can
explain the state of the results before I arrived in chapter 5. In chapter 6 I
explain what I did for computing errorbars for non-linear function and at last
I explain the final results in chapter 7.

\vfill

\paragraph{Notations:} I'll use the bra-ket notation and thus my Hilbertian
products are linear on the right. On $\R^n$ I'll use $>$ and $\geq$ for
component wise inequality. On $M_n(\C)$, $A > B$ mean that $A-B$ is hermitian
positive definite and $A \geq B$ means that $A - B$ is hermitian positive
semi-definite. This works obviously also on $\mathcal{M}_n(\R)$. The conjugate will be
denoted by $A^*$, the transpose by $A^t$, and the transpose conjugate by $A^\dagger$.


\chapter{Experimental setup and goal}

In this chapter I expect a basic knowledge of quantum mechanics, Hilbert space and
Schrodinger equations. In the while report I will use the usual bra-ket notation
and therefore a scalar product linear to the right.

I will first describe the experiment in high-level point of view, then I'll
quickly dive into experimental details I'll finish by explaining why
reconstruction is needed.

\section{Basic setup description}

In this section I will describe the experiment I worked with at the LKB. This
experience studies the interaction between Rydberg circular atoms and light in
cavities.
We produce Rydberg atoms in a known state, then we send them through a
cavities with a very small number of resonating photons (usually less than
eight). The atom then goes through a detector that ideally do projectile
measurement of the state. The cavity can be tune via stark effect so the
interaction can be resonant (the cavity frequencies correspond to a gap between
states) or diffusive (The frequencies and the gap do not match).

I will present the formalism for each component and then detail the formalism of
the whole experiment. The goal of the experiment is to study quantum
thermodynamics. In particular, with the right manipulation of the quantum state
of an atom, we can make give heat to the cavity (emit a photon) even if the atom
is colder than the cavity. I will define the notion of temperature of single
atom later.

Most of the formulas come from \cite{Har06}


\subsection{Rydberg Ciruclar atoms}

The atoms are prepared in a specific circular Rydberg state called ground state
$\ket g$. It correspond the energy level $n = 50$. We will also use $\ket e$
called excited state which is the circular state at $n = 51$ and $\ket f$, the
``fundamental state'' which is at $n = 49$. In theory the atom should never get
in any other state during our protocol.
The gap between $\ket e$ and $\ket g$ is
approximately $\SI{51}{GHz}$, whereas the gap between $\ket g$ and $\ket f$ is
around $\SI{49}{GHz}$.

In most of our calculation we will manipulate our states by pairs, and not as the
whole triplet, because each interaction the atom will do with our setup, will be
done at, or near a resonance frequency. In such case, the Hamiltonian we will
use is:
\[H = \frac {\hbar \omega}2 \sigma_Z\]
 where $\omega$ is the gap frequency and $\sigma_Z$ in one of the
 three Pauli matrices used in two-levels systems:
\[\sigma_Z = \mat{1&0\\0&1} \quad \quad \sigma_X = \mat{0&1\\1&0} \quad \quad
  \sigma_Y = \mat{0&-i\\i&0}\]

It is important to note that the $X,Y,Z$ axis here have nothing to do with the
real world spatial axis but there are just a convention due to the fact that
this two-level system looks a spin-system, where those axis would have had a
spatial meaning.

From those operator we may also build the raising and lowering operators:
\[\sigma_+ = \frac{\sigma_X + i \sigma_Y}2 = \ket 1 \bra 0 \quad \quad \quad \quad
  \sigma_- = \frac{\sigma_X - i \sigma_Y}2 = \ket 0 \bra 1\]

Those operator are the equivalent of anihilation and creation operator in usual
harmonic oscillator (That I will present in \cref{ssec:rescav}). We can even
express the Hamiltonian in a similar way, $H = \hbar \omega(\sigma_+\sigma_- - \frac12)$




\subsection{Ramsay zone: Interaction with classical field}

In what we call Ramsay zone, the atom will be submitted to a classical oscillating
field, resonant with the transition. I'll use $\ket e$ and $\ket g$ has my
excited and normal states, but it works the same The field at the position of the atom will
be denoted by:

\[\bs E = \mathcal{E}
  \mat{u_x\sin(\omega_f t + \varphi + \varphi_x)\\
    u_y\sin(\omega_f t + \varphi + \varphi_y)\\
    u_z\sin(\omega_f t + \varphi + \varphi_z)}\]
where $u_x^2 + u_y^2 +u_z^2 = 1$. We do not merge the global phase $\phi$ into
the component phase to be able to use it later. This can be represented more
cleanly using $\bs u_f = \big(u_x e^{i\varphi_x},u_y e^{i\varphi_y},u_z
e^{i\varphi_z}\big)$:
\[\bs E = i\mathcal{E} (\bs u_f\, e^{-i\omega_f t-\varphi} - \bs u_f^*\,
  e^{i\omega_f t + \varphi})\]

With that field we can now express the Hamiltonian $H_f$ of the
interaction between the electric dipole of the
atom and the electric field. It is $H_f = - \bs D \cdot \bs E$.
The dipole operator has a simple expression in terms
of the position operator $\bs R$ of the electron which is $\bs D = q \bs R$.
In our circular states, we have $\bra e R \ket e = 0$ and $\bra g R \ket g = 0$,
so we only care about the off diagonal terms. We want to write the dipole
operator as:
\[\bs D = d\mat{\bs 0&\bs u_a^*\\\bs u_a&\bs 0}\]
Such that $\bs u_a$ expressed the direction and $d$ the value of dipolar
moment. They are defined by $ q \bra g \bs R \ket e = d \bs u_a$. The field
Hamiltonian has now the expression:

\[ H_f = id\mathcal{E} \mat{\bs 0 &
  \bs u_a^* \cdot (\bs u_f\, e^{-i\omega_f t-\varphi} - \bs u_f^*\,
  e^{i\omega_f t + \varphi})\\
\bs u_a \cdot (\bs u_f\, e^{-i\omega_f t-\varphi} - \bs u_f^*\,
  e^{i\omega_f t + \varphi}) & \bs 0
}\]

\

In order to express the evolution of the system more easily, we will place
ourselves in an interaction representation. That means that we remove the
evolution of an Hamiltonian from the

\subsection{Resonant cavities}\label{ssec:rescav}

Harmonic oscillator.

\subsection{Atom-cavity interaction}

\subsection{Protocol}

From Luis notes.


\section{Experimental details and calibration}

From Clement's thesis

\subsection{State preparation}

% oven
% speed filtering Laser with doppler + resonnance
% State filtering (n=52)
% We ignore non-Rydeberg atoms

\subsection{Detection of atom}

% see thesis for functioning of detector

\subsection{Cooling and relaxation times}

% Say some stuff about cooling

\section{Reconstruction}

Still from~\cite{Har06} and Clement's thesis

\subsection{Ramsay interferometer}

\subsection{Phase shift of QND atoms}

\subsection{What data we have as an output}

Personal





\chapter{Maximum likelihood reconstruction}

I put some reminders about multivariate covariances in
\cref{app:cov}, which could be useful in this chapter.

\section{Reminders about estimation theory}

\subsection{Estimator}

The goal of estimation is to estimate a parameters using observations or
measurements. In the simplest case, we measure an random output $X \in \mathcal{X}$ from an
unknown input $\theta \in \Theta$. We place ourselves in a model where the law
of X condition to $\theta$ is known. Formally, We have for each $\theta$ a
probability measure $P_\theta$ on $\mathcal{X}$. If $P_\theta$ has the right
form (in particular if $X$ is a vector of repetition of the same experiment), we
can extract information on $\theta$ from $X$. For that we can use an estimator.

\begin{defn}
  An \emph{estimator} of parameter $\theta$ from $X$ is just a deterministic
  function $\hat \theta(X)$ that tries to give the most likely $\theta$ that
  could have outputted $X$.
\end{defn}

\begin{rem}
  The estimator notation $\hat \theta$ has noting to do with the notation of
  quantum operator in quantum mechanics. It should usually be clear from the
  context which one it is.
\end{rem}

\

The most current case of parameter is $\theta \in \R$. In that case we can
easily compare the original $\theta$ with its estimator $\hat \theta$ with a
subtraction:

\begin{defn}
  For a given $\theta$, the \emph{bias} of the estimator $\hat \theta$ is:
  \[B(\hat{\theta}) = E_\theta(\hat{\theta}(X) - \theta)\]
  where the expectation $E_\theta$ is taken on the law $P_\theta$.
\end{defn}

\begin{defn}
  For a given $\theta$, the \emph{variance} of the estimator $\hat \theta$ is:
  \[V(\hat{\theta}) = E_\theta\Big({(\hat{\theta}(X) - \theta)}^2\Big)\]
\end{defn}

\subsection{Likelihood}

In the case where all probability measure $P_\theta$ are absolutely
continuous against a ``canonincal'' measure $\mu$ on $\mathcal{X}$, we can
define a function $f(x;\theta)$ such that
\[P_\theta(A) = \int_A f(x;\theta)\,\dd \mu(x)\]

\begin{defn}
  For a given observation $x \in \mathcal{X}$, we define the \emph{likelihood} function
  $\mathcal{L}_x(\theta) = f(x;\theta)$.
\end{defn}

\begin{defn}
  For a given observation $x \in \mathcal{X}$, we define the \emph{log-likelihood} function
  $\ell_x(\theta) = \ln \mathcal{L}_x(\theta)$.
\end{defn}

\subsection{Fischer information and Cramér-Rao bound}

We now assume a set of real parameter $\theta \in \mathcal{D} \subset \R^n$.

A way of knowing if we could improve our estimation at a specific point is the
gradient of the log-likelihood function. This is called the
\emph{score}. We have $s_x(\theta) = \partial_\theta \ell_x(\theta)$. The
lower the score (its norm, actually), the better our estimation.

As we move around the optimal likelihood value TODO bullshit

\begin{prop} The expectation of the score is 0.
\end{prop}
\begin{proof}
  TODO
\end{proof}

We can then study the variance of score. The lower this variance is, The more
$\theta$ value TODO bullshit

We can then make an important remark

\begin{lem}\label{lem:corscr}
  The covariance of the score and any unbiased estimator is the identity.
  Formally: $\cov(\hat \theta, s_x(\theta)) = I$
\end{lem}

\begin{proof}
\end{proof}

In light of the theorem about bounds of \cref{sec:correl}, we can give special
interest to the variance of the score and its link with the variance of any estimator

\begin{defn} The \emph{Fischer Information} of the parameter $\theta$ is defined
  as the variance of the score:
  \[I(\theta) = V\big(s(\theta)\big)\]
\end{defn}

If $\theta \in \R$, this is just the expectation of the square of the score, but
otherwise. $I(\theta)$ is a covariance matrix thus at least positive
semi-definite.
This information value is putting a bound on the minimum variance of any
unbiased estimator which is called the Cramér–Rao bound.

\begin{thm}[Cramér–Rao bound]
  For any un-biased estimator $\hat \theta$, If $I(\theta) > 0$, we have
  \[V(\hat{\theta}) \ge {I(\theta)}^{-1}\]
\end{thm}

\begin{rem}
  In the multivariate case, the ${}^{-1}$ obviously means the inverse of the matrix
\end{rem}

\begin{proof}
  The bound on multivariate correlation is proven in \cref{thm:correln}. The
  bound is written as:
  \[\cov(X,Y){V(Y)}^{-1}\cov(Y,X) \leq V(X)\]

  As $I(\theta) = V(s_x(\theta))$, and thank to \cref{lem:corscr}, the bound holds.
\end{proof}

In order to have a better interpretation of Fischer information, if the
likelihood is at least $\class 2$ in $\theta$, we can rewrite
it as a hessian:

\begin{prop}
  Under sufficient regularity assumption,
  the Fischer information can also be written as the opposite of the Hessian of the
  log-likelihood function.
  \[I(\theta) = -\frac{\partial^2 \ell}{\partial \theta^2}(\theta)\]
\end{prop}

\begin{proof}
  TODO
\end{proof}







\section{Maximum likelihood estimator}

\subsection{Definition}

Now that we have proven the best variance possible, We still have to build an
estimator that approach this bound. There are multiple kind of estimator. The
one mostly used is the average when $X = (X_1,\ldots,X_n)$ and
$E_\theta(X_i) = \theta$ and $V_\theta(X_i)$ is small enough. However in more
complex case this wont work. The goal of the estimator is to produce the more
likely $\theta$, ideally the maximum of a density function $p(\theta | X)$.
Thanks to bayes law, we have (if all measure can be expressed as density
function \textbf{TODO} put that first):
\[p(\theta|x) = p_\theta(x) \frac {p(\theta)}{p(x)}
  = p_\theta(x) \frac {p(\theta)}{\int p_{\theta'}(x)p(\theta')\,\dd \theta'}\]

And our goal would be to maximize $p(\theta|x)$. However,in order for that
formula to make sense, we need a prior law on $\theta$. If the prior
distribution is more or less uniform, it is the same as maximizing $p_\theta(x)
= \mathcal{L}_x(\theta)$.

\renewcommand{\ml}{_{ML}}

\begin{defn}
  The \emph{maximum likelihood estimator} $\theta\ml$ is defined by:
  \[\theta\ml(x) = \argmax \mathcal{L}_x\]
\end{defn}

The maximum likelihood do not have excessively nice properties in its own, but
when $X = (X_1,\ldots,X_n)$ where all the $X_i$ come from the same law
$P_\theta(X_0)$ and are independent and $n \to \infty$,
we will have nice convergence properties. In particular, it will converge in
probability to the right
$\theta$ and saturate the Cramér-Rao bound as $n \to \infty$. I will prove that
claim in our specific context in chapter 6 \textbf{TODO} proper reference.

\

In practice, as most probability laws are log-concave (in particular the normal
law), we will use the log-likelihood, $\ell_x$ instead of $\mathcal{L}_x$. Apart
from concavity, this also has the advantage that multiple repetition are
additive instead of multiplicative:
\[\mathcal{L}_x(\theta) = \prod_i \mathcal{L}_{x_i}(\theta) \quad \quad \quad
  \quad \ell_x(\theta) = \sum_i \ell_{x_i}(\theta)\]

In particular, as probability goes down to zero quickly and we will be working on
64 bits machine floating point numbers. There is a high risk of $\mathcal{L}$ to
be simply computed to zero on the machine when it is just really small. For
information the natural logarithm of the minimum positive 64 bits floating point number
is $-740$.

\subsection{Informal properties}

Put the probability function according to Vincent's thesis.

Add the case at the edge of $\mathcal{D}$.

\chapter{Effect matrices computation}

In this chapter, we stay in Hilbert spaces of finite dimension. Most of the
result can be extended to Hilbert spaces of countable dimension, by adding
some restriction like only using compact operators,
but we won't need it in the rest of the report.

\section{Density matrices}

\subsection{Justification}

In order to make significant observation on an experimental setup that has a lot
of errors everywhere, we need to make averages and statistics on quantum state.
One way of doing that would be to manipulate probability measure over state
space which is the unit sphere $S$ of the model Hilbert space $\mathcal{H}$. However if we look at
it in details, there are a lot of distributions that indistinguishable by any
measurement. In first approximation, we say that two probability distribution
$\lambda$ and $\mu$ on $S$ are indistinguishable,
if for any observable $A$ (Hermitian matrix) on $\mathcal{H}$, we
have:

\[\int_S \bra \phi A \ket \phi\,\dd \lambda(\phi) = \int_S \bra \phi A \ket
  \phi\,\dd \mu(\phi) \]

However, if we rewrite it, we have:

\[\int_S \bra \phi A \ket \phi\,\dd \lambda(\phi)
  = \int_S \Tr\big(\ket \phi \bra \phi A\big) \,\dd \lambda(\phi)
  = \Tr\left(\int_S \ket \phi \bra \phi \,\dd \lambda(\phi) A \right)\]

\begin{defn} We define the density matrix of a probability distribution $\lambda$ on
  quantum states:
  \[ \rho = \int_S \ket \phi \bra \phi \,\dd\lambda(\phi) \]
\end{defn}
\begin{prop}
  The average of an observable $A$ in any probability distribution $\lambda$ is
  given by $\Tr(\rho A)$
\end{prop}

\begin{rem} This is sufficient to fully characterise the distribution, because
 any observable $A$ can be written $A = \sum_a aP_a$, where $P_a$ are orthogonal
 projector who are also observable. The average of $P_a$ is $\Tr(\rho P_a)$ and
 thus for all the distribution that have density matrix $\rho$, The probability
 of output $a$ when we measure $A$ is the same.
\end{rem}

Therefore in the rest of this report, we will use density matrix as the only
representation of quantum distribution, as two distribution that share the same
density matrix are completely indistinguishable by any measurement. In practice,
as quantum physicist, manly manipulate probabilistic quantum state and not that
much usual \emph{ket} state, the term quantum state is used for a statistical
mixture described by a density matrix. A state of the form $\ket \phi$ is thus
called a \emph{pure} state.

\subsection{Properties}

We can now study what are density matrices and find necessary and sufficient
conditions for a matrix to be density matrix.

\begin{prop}\label{prop:dens}
  A density matrix is Hermitian positive ($\rho > 0$) of trace 1
\end{prop}

\begin{proof}
  $\ket \phi \bra \phi > 0$, so $ \int_S \ket \phi \bra \phi \,\dd\lambda(\phi)
  > 0$.
  $\Tr(\ket \phi \bra \phi) = 1$, so $\Tr(\rho) = \lambda(S) = 1$
\end{proof}

If we want to prove that is sufficient we need to look at the spectral
decomposition.
In finite dimension $n$, a spectral decomposition of $\rho$ give a
collection of eigen values $p_1, \ldots, p_n$ whose sum is~1. Those are the
analogous of probabilities in classical probability theory. The density matrix
can then be written:
\[\rho = \sum p_i \ket {\phi_i} \bra {\phi_i}\]

\begin{thm} The condition~\ref{prop:dens} is necessary and sufficient i.e any Hermitian positive matrix of trace 1 is a density matrix.
\end{thm}

\begin{proof}
  Any matrix satisfying the condition~\ref{prop:dens} can be decomposed in
  $\rho = \sum p_i \ket {\phi_i} \bra {\phi_i}$, and thus the atomic measure
  $\lambda(\{\ket {\phi_i}\}) = p_i$ give exactly that density matrix. This
  still work exactly the same way for Hermitian compact operators of trace one in infinite countable dimension.
\end{proof}

The set of density matrix will be denoted by $\mathcal{D}(\mathcal{H})$ (or just
$\mathcal{D}$ when there is no ambiguity). It is a subset of the set of
operators on $\mathcal{H}$, $\mathcal{O}(\mathcal{H})$ (only compact operators
in infinite countable dimension).

\subsection{Rank and purity}

In finite dimension, we can also look at the
rank of the density matrix that gives a lot of information).

\begin{prop}
  The rank of the density matrix $\rho$ is the dimension of the support of any
  probability distribution that can give this matrix
  (with the convention $\dim A = \dim \Span(A)$).
\end{prop}

\begin{proof}
   \TODO
\end{proof}

\begin{defn}
  If a density matrix $\rho$ has rank one, it only represents one unique state and can be written
  $\rho = \ket \phi \bra \phi$. It is thus called a \emph{pure state}. No
  difference is made between the state $\ket \phi$ and the density matrix $\ket
  \phi \bra \phi$ as both are called pure states.

  On the other hand if $\rk \rho > 1$, the density matrix represent a
  \emph{mixed state} : a statistical mixture of state.
\end{defn}

\begin{rem}
  It is very important to differentiate the quantum superposition, for example
  on qubit,
  $\ket + = \frac1{\sqrt 2} \ket 0 + \frac1{\sqrt2} \ket 1$ and a statistical mixture of
  $50$ \% of $\ket 0$ and $50$\% of $\ket 1$. When the state in measured in base
  $\ket0,\ket1$, the results will be the same but in other base they could be
  different. In particular, in base $\ket +, \ket -$, the first one will be
  measured as $100$ \% $\ket +$ and $0$ \% $\ket -$ but the second will still be
  50-50 of $\ket+$ and $\ket -$.
\end{rem}

\begin{prop}
  A state $\rho$ is pure iff $\Tr \rho^2 = 1$.
\end{prop}

\begin{proof}
   \TODO
\end{proof}

The value $\Tr \rho^2$ has in fact much more interesting properties, and thus
deserves a name.

\begin{defn}
  The \emph{purity} of a quantum state represented by $\rho$ is the value $\gamma = \Tr \rho^2$.
\end{defn}

\begin{prop}
  In dimension $d$, the purity is always between $\frac1d$ and $1$. If $\gamma =
  \frac1d$, then $\rho = I$ and the mixture represented is uniform, we have
  absolutely no information on the state. If $\gamma = 1$, The state is pure and
  we thus have complete information on the quantum state.
\end{prop}

\begin{proof}
   \TODO
\end{proof}

The main interest of purity is that in unitary evolution ($\Tr (U^\dagger \rho U)
= \Tr \rho^2$, it remains constant,
whereas it will decrease in any other evolution, relaxation, decoherence,
measurement, as we lose information.

\subsection{Coefficients interpretation}

When we express the density matrix in a specific base, we gain a lot of
information about the state projected in that base, in particular about how
projective measurements in that base will happen.

\begin{defn}
  When $\rho$ is express in the base $\ket 1, \ldots, \ket d$, the diagonal
  coefficients which are real, are called \emph{populations}, because the
  coefficient $\bra i \rho \ket i$ is the probability of measuring state $i$,
  when measuring in that base.

  The non-diagonal coefficient are complex and named \emph{coherence}. The
  coherence between state $\ket i$ and state $\ket j$ is $\bra i \rho \ket j$
  and is the conjugate of the coherence between $\ket j$ and $\ket i$.
\end{defn}

The point of this distinction is that any projective measurement made in that
base will have the same statistics whatever the value of coherences. Their
values, will only matter when measuring value in other bases or when the state
is evolving. This also means reciprocally, in the context of tomography,
that measurements made in that base can bring no information.

As we will reconstruct $\rho$, numerically we will use this matrix
representation, we need to study the structure of $\mathcal{D}(\mathcal{H})$ in
this form. In particular:

\begin{prop}
  $\dim \mathcal{D}(\mathcal{H}) = d^2 - 1 $ as a real vector space.
\end{prop}

\begin{proof}
  The (real) dimension of Hermitian matrices is $d^2$ because there are $d$ diagonal
  coefficient and $\frac {d(d-1)} 2$ complex off-diagonal coefficient so $d +
  d(d-1)$ coefficient in total. The constraint $\Tr \rho = 1$ remove one degree
  of freedom, so we only have $d^2 -1$ dimension.
\end{proof}

\subsection{The case of the qubit}

In the case of a qubit, with $\dim \mathcal{H} = 2$ (complex dimension), we know
that the pure states, up to a phase shift can be represented on the Bloch sphere. We
would like to know how to represent mixed states. The dimension of
$\mathcal{D}(\mathcal{H})$ is 3, so we can hope for a simple representation. Any
density matrix from $\mathcal{H}$ can be written:

\[\rho = \frac12I + S\]

where $\Tr S = 0$. A base of the hyperplane of hermitian matrix of trace $0$ can
be given by Pauli matrices :


\[\sigma_Z = \mat{1&0\\0&1} \quad \quad \sigma_X = \mat{0&1\\1&0} \quad \quad
  \sigma_Y = \mat{0&-i\\i&0}\]

If we write $\bs\sigma = (\sigma_X,\sigma_yY,\sigma_Z)$, we can write
$\rho$ as :

\[\rho = \frac12 I + \bs r \cdot \bs \sigma\]

where $\bs r \in \R^3$ is usual 3D vector. We can check  However we have :

\begin{prop}
  $\rho \geq 0$ if and only if $\|\bs r\| \leq 1$
\end{prop}

\begin{proof}
  $\det \rho = 1 - \|r\|^2$
\end{proof}

We can then look at the purity $\gamma$ in function of $\bs r$. We have :

\begin{prop}
  $\gamma = \Tr(\rho^2) = \frac 12 (1 + \|\bs r\|^2)$
\end{prop}

\begin{cor}
  $\rho$ is pure if and only if $\|\bs r\| = 1$
\end{cor}

Mixed state are thus represented in Bloch ``ball''. The pure states are exactly
the one that are laying on the boundary i.e. on the sphere. One can check this
representation matches the usual Block sphere representation on pure states. The
uniform state is the center of the ball.

\section{Krauss Maps}

\section{Likelihood in terms of Krauss maps}

Mainly~\cite{SPRQT16} but some others

\section{Computation of Krauss operators for QND measurement}
Mostly from~\cite{VM19} but with a bit of~\cite{Har06}

\chapter{Convex optimization}

\section{Reminders on convex optimization}

Aspremont convex optimization course

Formal problem, and dual

\section{Solution characterization}

Application of KKT

\section{Projected gradient method}

Why it converges,

How to do the projection.

\section{Convergence criterion}

\paragraph{TODO} Ask Rouchon or understand.

\chapter{First Results}
\section{Extracting the results and basic error}

Mention the results already proven and put a plot of reconstructed state with
the errorbars we already had.

We get error on matrix value and photon number

\section{Maxwell Demon experiment}

% Here or first chapter
Mainly Luis notes and presentation


\section{Reminders on Quantum information}

Wikipédia

\subsection{Classical Information theory}

\subsection{Quantum information theory}

\section{Thermodynamical analysis}

Built the new second law from

Mainly the long notes from Luis


\section{Experimental results}

\paragraph{Combination of reconstructions} How we build the matrices
experimentally : from code

Plots and error bars ?

\paragraph{TODO} Need to save the data needed for the report

\chapter{Error estimation and validity proofs}
\section{Quick approach : Monte carlo estimation}
% Only on specific case of probability vectors
\subsection{Reminders on Markov chains}

Continuous Markov chain notes

\subsection{Truncated gaussian simulation}
% Convergence proofs : L O L !

\section{Probability distribution around maximum}

Full rank and partial rank case

Give the formula that we will prove by intuition

Will probably need the transition from the z hessian to the usual notation here.

\section{Complete first order error-bars}

Use \cite{SPRAL17} but generalise. Will try to compact the proofs

Maybe some stuff will go in the appendices
\subsection{Full rank}

\subsection{Low rank}

\section{Fix the non full span problem}

\subsection{Centering}

Explain approach coefficient by coefficient

Prove the logarithmic barrier

\subsection{Two objective optimisation}

Compare approachs and prove them :
\begin{itemize}
  \item Projected gradient on argmax
  \item Central path method
\end{itemize}

\chapter{Final Results}

\section{Approximations and implementation}

Gradient of entropy, projections, ...

Relevant implementation details

Equations of covariance propagation. All first order error.

\section{Plots}

\paragraph{TODO} Gather data for plots

\section{Interpretation}

Well crafted bullshit

\chapter*{Conclusion} %and bibliography
\addcontentsline{toc}{chapter}{Conclusion - Thanks - Bibliography}

Conclusion



\vfill

\paragraph{\Huge Thanks}
% \addcontentsline{toc}{chapter}{Thanks}

\

\vspace{3mm}

\begin{itemize}

\item Thanks to Igor Dotsenko for welcoming me in his lab, showing me his
  experimental setup and giving me interesting things to do around it.
\item Pierre Rouchon
\item Valentin Metillon
\item Luis Najera

\end{itemize}

\vfill


% Bibliography

% \addcontentsline{toc}{chapter}{Bibliography}

\bibliographystyle{plain}

{\let\clearpage\relax \bibliography{report}}


\appendix

\chapter{Quantum mechanics pictures}

Picture are different way to perceive and express quantum mechanics. They vary
in how the system state and the various operator change over time. All those
representation yield the same mechanics and are equivalent up to certain change
of Hilbert space basis dependent on time. Going from one to the other is just up
to a unitary $U(t)$.

However, non-unitary evolution like decoherence or external measurement
cannot be moved like that because it is not inversible. Therefore such
evolutions will always apply to state.

\section{Schrodinger picture}

The Schrodinger picture is the usual representation of quantum mechanics. In
this context only the state of the system varies and the way to observe it do
not change. When the system evolves under Hamiltonian $H(t)$,
the various operators relevant to the system are constant and the
state follow the Schrodinger equation:
\[i\hbar \frac{\partial \ket \Psi}{\partial t} (t) = H(t) \ket {\Psi(t)} \]




\section{Heisenberg picture}


\section{Interaction picture}

Sometimes, in order to compare two phenomenon, it is useful to compare two
Hamiltonian in certain way. Usually, this is to compare the free evolution of a
system to the interaction with another system, this is why this is called the
interaction picture.


\chapter{Covariance}\label{app:cov}

\section{Covariance and variance}

\begin{defn}
  Given two random vector $X \in \R^n$ and $Y \in \R^m$, we let the covariance
  matrix be defined by:
  \[\cov(X,Y) = {\big(\cov(X_i,Y_j)\big)}_{i \leq n, j \leq m}\]
\end{defn}

The fundamental goal of the covariance matrix is to have $\bra a \cov(x,y) \ket
b = \cov(a \cdot x, b \cdot y)$
\begin{prop} If $X \in \R^n$ and $Y \in \R^m$ are two random vectors, we have $\cov(X,Y) = {\cov(Y,X)}^t$.
\end{prop}
\begin{prop} If $A$ is a deterministic matrix, we have $ \cov(A X, Y) = A \cov(X,Y)$
\end{prop}

\begin{defn}
  Given a random vector $X \in \R^n$, We define its variance by
  \[V(X) = \cov(X,X)\]
\end{defn}

\begin{prop} For $X \in \R^n$a random vector, we have $V(X) > 0$
\end{prop}

\section{Correlation}\label{sec:correl}

\begin{defn}
  If $X$ and $Y$ are two real valued variable that are not fixed (non-zero
  variance), we have:
  \[\rho = \left|\frac {\cov(X,Y)}{\sigma(X),\sigma(Y)}\right|\]
  where $\sigma(X) = \sqrt{V(X)}$
\end{defn}

\begin{prop}\label{prop:correl1}
  We always have: $|\rho| \le 1$
\end{prop}

We can extend the correlation to the multivariate case by setting
\[\rho = {V(X)}^{-\frac12}\cov(X,Y){V(Y)}^{-\frac12}\]
The norm now become the symmetric part of the polar decomposition noted $|\rho|
= {(\rho\rho^t)}^{\frac12}$. We can now get the theorem that but bound on the
correlation:

\begin{thm}\label{thm:correln}
  If $X \in \R^n$ and $Y \in \R^m$ are random vectors, and $V(Y)$ is invertible,
  we have:
  \[\cov(X,Y){V(Y)}^{-1}\cov(Y,X) \leq V(X)\]
\end{thm}

\begin{cor}
  If $V(X)$ is also invertible, we have $|\rho| < I$
\end{cor}

\begin{proof}
  We take $a \in R^n$ such that $\bra a V(X) \ket a > 0$ and any $b \in \R^m$.
  By using~\cref{prop:correl1}, we get:
  \[ \frac{ \bra a \cov(X,Y) \ket b}{\sqrt{\bra a V(X) \ket a}\sqrt{\bra b V(Y)
        \ket b}} \leq 1\]
  To saturate the inequality, we can maximize the numerator on $b$, but keep the
  denominator constant. If we use the Lagrange multiplier method, we want to
  find a critical point of:
  \[\mathcal{L}(b,\lambda) = \bra a \cov(X,Y) \ket b + \lambda \bra b V(Y) \ket
    b\]

  We thus need to have:
  \[ \bra a \cov(X,Y) + 2\lambda\bra b V(Y) = 0\]
  and thus:
  \[ \bra b = - \frac 1 {2\lambda} \bra a \cov(X,Y) {V(Y)}^{-1}\]
  By substituting in the first equation we get:
  \[ \frac{ \bra a \cov(X,Y) {V(Y)}^{-1} \cov(Y,X) \ket a}
    {\sqrt{\bra a V(X) \ket a}
      \sqrt{\bra a \cov(X,Y) {V(Y)}^{-1} \cov(Y,X) \ket a}} \leq 1\]
  We thus get for all $a$ such that $\bra a V(X) \ket a > 0$:
  \[ \bra a \cov(X,Y) {V(Y)}^{-1} \cov(Y,X) \ket a < \bra a V(X) \ket a\]
  As for other $a$, both terms are 0, this inequality holds for all $a$, thus
  the theorem holds.
\end{proof}



\section{First order approximation}


\end{document}
